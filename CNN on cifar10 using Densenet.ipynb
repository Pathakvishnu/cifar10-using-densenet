{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use Dense Layers (also called fully connected layers), or DropOut.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import SeparableConv2D,Conv2D, Dense, Input, add, Activation, AveragePooling2D, GlobalAveragePooling2D, Lambda, concatenate\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train,y_train),(X_test,y_test)=cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = np.load('data.npz')['a']\n",
    "X_test = np.load('data.npz')['b']\n",
    "y_train = np.load('data.npz')['c']\n",
    "y_test = np.load('data.npz')['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 932 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "y_train = to_categorical(y_train,10)\n",
    "y_test = to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_scheduler(epoch):\n",
    "    if epoch < 150:\n",
    "        return 0.1\n",
    "    if epoch < 225:\n",
    "        return 0.01\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def conv(x, out_filters, k_size):\n",
    "    return(SeparableConv2D(filters=out_filters,kernel_size=k_size,strides=(1,1), padding='same',kernel_initializer='he_normal', \n",
    "                  kernel_regularizer=l2(weight_decay),use_bias=False)(x))\n",
    "\n",
    "def dense_layer(x):\n",
    "    return(Dense(units=classes_num,\n",
    "                    activation='softmax',\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(weight_decay))(x))\n",
    "\n",
    "def bn_relu(x):\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return(x)\n",
    "\n",
    "def bottleneck_layer(x):\n",
    "    channels = growth_rate * 4\n",
    "    x = bn_relu(x)\n",
    "    x = conv(x, channels, (1,1))\n",
    "    x = bn_relu(x)\n",
    "    x = conv(x, growth_rate, (3,3))\n",
    "    return(x)\n",
    "\n",
    "def single(x):\n",
    "    x = bn_relu(x)\n",
    "    x = conv(x, growth_rate, (3,3))\n",
    "    return(x)\n",
    "\n",
    "def transition(x, inchannels):\n",
    "    outchannels = int(inchannels * compression)\n",
    "    x = bn_relu(x)\n",
    "    x = conv(x, outchannels, (1,1))\n",
    "    x = AveragePooling2D((2,2), strides=(2, 2))(x)\n",
    "    return(x, outchannels)\n",
    "\n",
    "def dense_block(x,blocks,nchannels):\n",
    "    concat = x\n",
    "    for i in range(blocks):\n",
    "        x = bottleneck_layer(concat)\n",
    "        concat = concatenate([x,concat], axis=-1)\n",
    "        nchannels += growth_rate\n",
    "    return(concat, nchannels)\n",
    "def densenet(img_input,classes_num,depth,growth_rate,compression):\n",
    "    #nblocks = (depth - 4) // 6 \n",
    "    nchannels = growth_rate * 2\n",
    "    x = conv(img_input, nchannels, (3,3))\n",
    "    x, nchannels = dense_block(x,nblocks,nchannels)\n",
    "    x, nchannels = transition(x,nchannels)\n",
    "    x, nchannels = dense_block(x,nblocks,nchannels)\n",
    "    x, nchannels = transition(x,nchannels)\n",
    "    x, nchannels = dense_block(x,nblocks,nchannels)\n",
    "    x = bn_relu(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = dense_layer(x)\n",
    "    return(Model(img_input,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rate= 12 \n",
    "depth = 64\n",
    "compression = 0.5\n",
    "batch_size = 64 \n",
    "nb_epochs = 300\n",
    "iterations = 200\n",
    "classes_num = 10\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0926 23:00:11.588988 16052 deprecation_wrapper.py:119] From C:\\Users\\patha\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 24)   99          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 32, 32, 48)   1176        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 48)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 32, 32, 12)   1008        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           separable_conv2d_3[0][0]         \n",
      "                                                                 separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 36)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 32, 32, 48)   1764        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 32, 32, 12)   1008        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           separable_conv2d_5[0][0]         \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 48)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 32, 32, 48)   2352        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 32, 32, 12)   1008        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           separable_conv2d_7[0][0]         \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 60)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 32, 32, 48)   2940        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 48)   192         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 48)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 32, 32, 12)   1008        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           separable_conv2d_9[0][0]         \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 72)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 32, 32, 48)   3528        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 48)   192         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 48)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 32, 32, 12)   1008        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           separable_conv2d_11[0][0]        \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 84)   336         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 84)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 32, 32, 48)   4116        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 48)   192         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 48)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 32, 32, 12)   1008        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           separable_conv2d_13[0][0]        \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 96)   384         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 96)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 32, 32, 48)   4704        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 48)   192         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 32, 32, 12)   1008        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           separable_conv2d_15[0][0]        \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 108)  432         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 108)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 32, 32, 48)   5292        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 48)   192         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 48)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 32, 32, 12)   1008        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           separable_conv2d_17[0][0]        \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 120)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 32, 32, 48)   5880        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 48)   192         separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 48)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 32, 32, 12)   1008        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           separable_conv2d_19[0][0]        \n",
      "                                                                 concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 32, 132)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 32, 32, 48)   6468        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 48)   192         separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 32, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 32, 32, 12)   1008        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           separable_conv2d_21[0][0]        \n",
      "                                                                 concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 32, 144)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 32, 32, 72)   10512       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 72)   0           separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 72)   288         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 72)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 16, 16, 48)   3528        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 48)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 16, 16, 12)   1008        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 84)   0           separable_conv2d_24[0][0]        \n",
      "                                                                 average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 84)   336         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 84)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 16, 16, 48)   4116        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 48)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 16, 16, 12)   1008        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 96)   0           separable_conv2d_26[0][0]        \n",
      "                                                                 concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 96)   384         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 96)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 16, 16, 48)   4704        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 48)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 16, 16, 12)   1008        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 108)  0           separable_conv2d_28[0][0]        \n",
      "                                                                 concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 108)  432         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 108)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 16, 16, 48)   5292        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 48)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 16, 16, 12)   1008        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 120)  0           separable_conv2d_30[0][0]        \n",
      "                                                                 concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 120)  480         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 120)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 16, 16, 48)   5880        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 48)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 16, 16, 12)   1008        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 132)  0           separable_conv2d_32[0][0]        \n",
      "                                                                 concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 132)  528         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 132)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_33 (SeparableC (None, 16, 16, 48)   6468        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 48)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_34 (SeparableC (None, 16, 16, 12)   1008        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 144)  0           separable_conv2d_34[0][0]        \n",
      "                                                                 concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 144)  576         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 144)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_35 (SeparableC (None, 16, 16, 48)   7056        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 48)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_36 (SeparableC (None, 16, 16, 12)   1008        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 156)  0           separable_conv2d_36[0][0]        \n",
      "                                                                 concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 156)  624         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 156)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_37 (SeparableC (None, 16, 16, 48)   7644        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 48)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_38 (SeparableC (None, 16, 16, 12)   1008        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 168)  0           separable_conv2d_38[0][0]        \n",
      "                                                                 concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 168)  672         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 168)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_39 (SeparableC (None, 16, 16, 48)   8232        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_39[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 16, 16, 48)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_40 (SeparableC (None, 16, 16, 12)   1008        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 180)  0           separable_conv2d_40[0][0]        \n",
      "                                                                 concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 16, 16, 180)  720         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 16, 16, 180)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_41 (SeparableC (None, 16, 16, 48)   8820        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 16, 16, 48)   192         separable_conv2d_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 16, 48)   0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_42 (SeparableC (None, 16, 16, 12)   1008        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 192)  0           separable_conv2d_42[0][0]        \n",
      "                                                                 concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 16, 16, 192)  768         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 16, 192)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_43 (SeparableC (None, 16, 16, 96)   18624       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 96)     0           separable_conv2d_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 8, 8, 96)     384         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 96)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_44 (SeparableC (None, 8, 8, 48)     4704        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 48)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_45 (SeparableC (None, 8, 8, 12)     1008        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 8, 8, 108)    0           separable_conv2d_45[0][0]        \n",
      "                                                                 average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 8, 8, 108)    432         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 108)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_46 (SeparableC (None, 8, 8, 48)     5292        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 8, 8, 48)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_47 (SeparableC (None, 8, 8, 12)     1008        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 8, 8, 120)    0           separable_conv2d_47[0][0]        \n",
      "                                                                 concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 120)    480         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 8, 8, 120)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_48 (SeparableC (None, 8, 8, 48)     5880        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 8, 8, 48)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_49 (SeparableC (None, 8, 8, 12)     1008        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 8, 8, 132)    0           separable_conv2d_49[0][0]        \n",
      "                                                                 concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 8, 8, 132)    528         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 8, 8, 132)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_50 (SeparableC (None, 8, 8, 48)     6468        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_50[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 8, 8, 48)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_51 (SeparableC (None, 8, 8, 12)     1008        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 144)    0           separable_conv2d_51[0][0]        \n",
      "                                                                 concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 8, 8, 144)    576         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 8, 8, 144)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_52 (SeparableC (None, 8, 8, 48)     7056        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_52[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 8, 8, 48)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_53 (SeparableC (None, 8, 8, 12)     1008        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 156)    0           separable_conv2d_53[0][0]        \n",
      "                                                                 concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 156)    624         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 8, 8, 156)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_54 (SeparableC (None, 8, 8, 48)     7644        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_54[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 8, 8, 48)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_55 (SeparableC (None, 8, 8, 12)     1008        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 168)    0           separable_conv2d_55[0][0]        \n",
      "                                                                 concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 8, 8, 168)    672         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 8, 8, 168)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_56 (SeparableC (None, 8, 8, 48)     8232        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_56[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 8, 8, 48)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_57 (SeparableC (None, 8, 8, 12)     1008        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 180)    0           separable_conv2d_57[0][0]        \n",
      "                                                                 concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 8, 8, 180)    720         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 8, 8, 180)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_58 (SeparableC (None, 8, 8, 48)     8820        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 8, 8, 48)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_59 (SeparableC (None, 8, 8, 12)     1008        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 192)    0           separable_conv2d_59[0][0]        \n",
      "                                                                 concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 8, 8, 192)    768         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 8, 8, 192)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_60 (SeparableC (None, 8, 8, 48)     9408        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_60[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 8, 8, 48)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_61 (SeparableC (None, 8, 8, 12)     1008        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 204)    0           separable_conv2d_61[0][0]        \n",
      "                                                                 concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 8, 8, 204)    816         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 8, 8, 204)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_62 (SeparableC (None, 8, 8, 48)     9996        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 8, 8, 48)     192         separable_conv2d_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 8, 8, 48)     0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_63 (SeparableC (None, 8, 8, 12)     1008        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 216)    0           separable_conv2d_63[0][0]        \n",
      "                                                                 concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 8, 8, 216)    864         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 8, 8, 216)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 216)          0           activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2170        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 257,233\n",
      "Trainable params: 246,169\n",
      "Non-trainable params: 11,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = densenet(Input(shape=X_train.shape[1:]),10,depth,growth_rate,compression)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_cb     = TensorBoard(log_dir='./densenet/', histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(learning_scheduler)\n",
    "ckpt      = ModelCheckpoint('./ckpt.h5', save_best_only=False, mode='auto', period=10)\n",
    "cbks      = [change_lr,tb_cb,ckpt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125,fill_mode='nearest',cval=0.)\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=.001, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0926 23:01:04.521609 16052 deprecation_wrapper.py:119] From C:\\Users\\patha\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0926 23:01:04.534597 16052 deprecation_wrapper.py:119] From C:\\Users\\patha\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0926 23:01:04.535587 16052 deprecation_wrapper.py:119] From C:\\Users\\patha\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "200/200 [==============================] - 72s 361ms/step - loss: 1.9063 - accuracy: 0.2834 - val_loss: 1.8889 - val_accuracy: 0.3041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0926 23:02:18.830979 16052 deprecation_wrapper.py:119] From C:\\Users\\patha\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 1.5894 - accuracy: 0.4134 - val_loss: 2.0820 - val_accuracy: 0.3291\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 1.3914 - accuracy: 0.4960 - val_loss: 1.4050 - val_accuracy: 0.5001\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 1.2474 - accuracy: 0.5541 - val_loss: 1.2844 - val_accuracy: 0.5363\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 1.1475 - accuracy: 0.5887 - val_loss: 1.5610 - val_accuracy: 0.5167\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 1.0964 - accuracy: 0.6190 - val_loss: 1.3549 - val_accuracy: 0.5353\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 1.0229 - accuracy: 0.6393 - val_loss: 1.1992 - val_accuracy: 0.5914\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.9655 - accuracy: 0.6654 - val_loss: 1.0213 - val_accuracy: 0.6503\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.9181 - accuracy: 0.6825 - val_loss: 0.9037 - val_accuracy: 0.6934\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.8732 - accuracy: 0.6948 - val_loss: 0.9438 - val_accuracy: 0.6927\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.8647 - accuracy: 0.7056 - val_loss: 0.8372 - val_accuracy: 0.7121\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.8004 - accuracy: 0.7191 - val_loss: 0.8248 - val_accuracy: 0.7212\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.7711 - accuracy: 0.7352 - val_loss: 0.8711 - val_accuracy: 0.7136\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.7532 - accuracy: 0.7420 - val_loss: 0.9547 - val_accuracy: 0.6943\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.7373 - accuracy: 0.7525 - val_loss: 0.7745 - val_accuracy: 0.7446\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.7144 - accuracy: 0.7591 - val_loss: 0.8613 - val_accuracy: 0.7247\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.7029 - accuracy: 0.7606 - val_loss: 0.8026 - val_accuracy: 0.7407\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6621 - accuracy: 0.7723 - val_loss: 0.8024 - val_accuracy: 0.7381\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6702 - accuracy: 0.7767 - val_loss: 0.8047 - val_accuracy: 0.7380\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6367 - accuracy: 0.7837 - val_loss: 0.7800 - val_accuracy: 0.7433\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6413 - accuracy: 0.7836 - val_loss: 0.6967 - val_accuracy: 0.7722\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.6109 - accuracy: 0.7920 - val_loss: 0.6584 - val_accuracy: 0.7820\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.6166 - accuracy: 0.7942 - val_loss: 0.6644 - val_accuracy: 0.7791\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5918 - accuracy: 0.8021 - val_loss: 0.6744 - val_accuracy: 0.7797\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5783 - accuracy: 0.8035 - val_loss: 0.6845 - val_accuracy: 0.7736\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5895 - accuracy: 0.8020 - val_loss: 0.6151 - val_accuracy: 0.8010\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.5568 - accuracy: 0.8131 - val_loss: 0.5854 - val_accuracy: 0.8090\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5364 - accuracy: 0.8207 - val_loss: 0.8365 - val_accuracy: 0.7445\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5426 - accuracy: 0.8184 - val_loss: 0.5873 - val_accuracy: 0.8079\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.5562 - accuracy: 0.8132 - val_loss: 0.6226 - val_accuracy: 0.7989\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5426 - accuracy: 0.8169 - val_loss: 0.6226 - val_accuracy: 0.7965\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5130 - accuracy: 0.8268 - val_loss: 0.5771 - val_accuracy: 0.8058\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5236 - accuracy: 0.8229 - val_loss: 0.7034 - val_accuracy: 0.7804\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5093 - accuracy: 0.8291 - val_loss: 0.6250 - val_accuracy: 0.7894\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.5075 - accuracy: 0.8313 - val_loss: 0.6063 - val_accuracy: 0.8022\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4845 - accuracy: 0.8355 - val_loss: 0.6792 - val_accuracy: 0.7815\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.5029 - accuracy: 0.8328 - val_loss: 0.5405 - val_accuracy: 0.8263\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4806 - accuracy: 0.8378 - val_loss: 0.6056 - val_accuracy: 0.8045\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4794 - accuracy: 0.8398 - val_loss: 0.5580 - val_accuracy: 0.8223\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4538 - accuracy: 0.8508 - val_loss: 0.5291 - val_accuracy: 0.8272\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4691 - accuracy: 0.8427 - val_loss: 0.6252 - val_accuracy: 0.7967\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4603 - accuracy: 0.8429 - val_loss: 0.6716 - val_accuracy: 0.7847\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.4753 - accuracy: 0.8415 - val_loss: 0.6663 - val_accuracy: 0.7864\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4428 - accuracy: 0.8555 - val_loss: 0.5029 - val_accuracy: 0.8361\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4393 - accuracy: 0.8585 - val_loss: 0.5025 - val_accuracy: 0.8399\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.4506 - accuracy: 0.8495 - val_loss: 0.5486 - val_accuracy: 0.8233\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4437 - accuracy: 0.8554 - val_loss: 0.5239 - val_accuracy: 0.8294\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4336 - accuracy: 0.8552 - val_loss: 0.5796 - val_accuracy: 0.8167\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4346 - accuracy: 0.8548 - val_loss: 0.6112 - val_accuracy: 0.8074\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4277 - accuracy: 0.8593 - val_loss: 0.5441 - val_accuracy: 0.8193\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4317 - accuracy: 0.8552 - val_loss: 0.4934 - val_accuracy: 0.8424\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4110 - accuracy: 0.8648 - val_loss: 0.5192 - val_accuracy: 0.8308\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.4067 - accuracy: 0.8649 - val_loss: 0.6001 - val_accuracy: 0.8172\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4162 - accuracy: 0.8583 - val_loss: 0.5002 - val_accuracy: 0.8366\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.4112 - accuracy: 0.8632 - val_loss: 0.5995 - val_accuracy: 0.8104\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3873 - accuracy: 0.8736 - val_loss: 0.4523 - val_accuracy: 0.8551\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.3913 - accuracy: 0.8698 - val_loss: 0.5052 - val_accuracy: 0.8322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4059 - accuracy: 0.8659 - val_loss: 0.6407 - val_accuracy: 0.7991\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.4101 - accuracy: 0.8644 - val_loss: 0.4718 - val_accuracy: 0.8484\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3843 - accuracy: 0.8713 - val_loss: 0.4835 - val_accuracy: 0.8452\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3694 - accuracy: 0.8774 - val_loss: 0.5543 - val_accuracy: 0.8283\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.4125 - accuracy: 0.8650 - val_loss: 0.4471 - val_accuracy: 0.8567\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3999 - accuracy: 0.8675 - val_loss: 0.5302 - val_accuracy: 0.8348\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3782 - accuracy: 0.8773 - val_loss: 0.4877 - val_accuracy: 0.8459\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3839 - accuracy: 0.8729 - val_loss: 0.4653 - val_accuracy: 0.8500\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3774 - accuracy: 0.8766 - val_loss: 0.4649 - val_accuracy: 0.8471\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3593 - accuracy: 0.8803 - val_loss: 0.4745 - val_accuracy: 0.8448\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3656 - accuracy: 0.8800 - val_loss: 0.5935 - val_accuracy: 0.8163\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.3671 - accuracy: 0.8789 - val_loss: 0.5546 - val_accuracy: 0.8275\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3740 - accuracy: 0.8740 - val_loss: 0.4833 - val_accuracy: 0.8481\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3564 - accuracy: 0.8835 - val_loss: 0.5357 - val_accuracy: 0.8310\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.3566 - accuracy: 0.8834 - val_loss: 0.4826 - val_accuracy: 0.8451\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.3469 - accuracy: 0.8861 - val_loss: 0.4535 - val_accuracy: 0.8534\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3481 - accuracy: 0.8855 - val_loss: 0.4946 - val_accuracy: 0.8448\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3499 - accuracy: 0.8802 - val_loss: 0.4740 - val_accuracy: 0.8511\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3580 - accuracy: 0.8804 - val_loss: 0.4789 - val_accuracy: 0.8406\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.3363 - accuracy: 0.8854 - val_loss: 0.4059 - val_accuracy: 0.8712\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3430 - accuracy: 0.8860 - val_loss: 0.4052 - val_accuracy: 0.8687\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3350 - accuracy: 0.8900 - val_loss: 0.4499 - val_accuracy: 0.8605\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3374 - accuracy: 0.8905 - val_loss: 0.4298 - val_accuracy: 0.8625\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.3381 - accuracy: 0.8910 - val_loss: 0.5176 - val_accuracy: 0.8315\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3356 - accuracy: 0.8884 - val_loss: 0.4333 - val_accuracy: 0.8604\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3215 - accuracy: 0.8921 - val_loss: 0.4153 - val_accuracy: 0.8677\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3311 - accuracy: 0.8913 - val_loss: 0.4356 - val_accuracy: 0.8650\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3344 - accuracy: 0.8909 - val_loss: 0.3953 - val_accuracy: 0.8711\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3278 - accuracy: 0.8920 - val_loss: 0.4980 - val_accuracy: 0.8418\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3142 - accuracy: 0.8966 - val_loss: 0.4635 - val_accuracy: 0.8539\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3265 - accuracy: 0.8916 - val_loss: 0.4719 - val_accuracy: 0.8526\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3197 - accuracy: 0.8955 - val_loss: 0.4452 - val_accuracy: 0.8615\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3255 - accuracy: 0.8908 - val_loss: 0.4178 - val_accuracy: 0.8653\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3055 - accuracy: 0.8985 - val_loss: 0.4402 - val_accuracy: 0.8598\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3035 - accuracy: 0.8998 - val_loss: 0.4184 - val_accuracy: 0.8693\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3234 - accuracy: 0.8950 - val_loss: 0.4584 - val_accuracy: 0.8523\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.3196 - accuracy: 0.8940 - val_loss: 0.3996 - val_accuracy: 0.8663\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.3018 - accuracy: 0.9048 - val_loss: 0.4398 - val_accuracy: 0.8594\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.3011 - accuracy: 0.9018 - val_loss: 0.4367 - val_accuracy: 0.8627\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3107 - accuracy: 0.8970 - val_loss: 0.4136 - val_accuracy: 0.8727\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3010 - accuracy: 0.9011 - val_loss: 0.3901 - val_accuracy: 0.8756\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2907 - accuracy: 0.9055 - val_loss: 0.4014 - val_accuracy: 0.8720\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2911 - accuracy: 0.9055 - val_loss: 0.4708 - val_accuracy: 0.8533\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3106 - accuracy: 0.8992 - val_loss: 0.4083 - val_accuracy: 0.8708\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2939 - accuracy: 0.9033 - val_loss: 0.4025 - val_accuracy: 0.8696\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2948 - accuracy: 0.9051 - val_loss: 0.4414 - val_accuracy: 0.8641\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2845 - accuracy: 0.9060 - val_loss: 0.4416 - val_accuracy: 0.8653\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.3043 - accuracy: 0.8965 - val_loss: 0.4000 - val_accuracy: 0.8758\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2852 - accuracy: 0.9059 - val_loss: 0.4174 - val_accuracy: 0.8699\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2873 - accuracy: 0.9046 - val_loss: 0.4118 - val_accuracy: 0.8729\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2877 - accuracy: 0.9019 - val_loss: 0.4234 - val_accuracy: 0.8699\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2867 - accuracy: 0.9069 - val_loss: 0.3928 - val_accuracy: 0.8740\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2808 - accuracy: 0.9075 - val_loss: 0.3914 - val_accuracy: 0.8807\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2801 - accuracy: 0.9101 - val_loss: 0.4030 - val_accuracy: 0.8781\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2878 - accuracy: 0.9074 - val_loss: 0.3912 - val_accuracy: 0.8791\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2984 - accuracy: 0.9032 - val_loss: 0.4235 - val_accuracy: 0.8655\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.2743 - accuracy: 0.9097 - val_loss: 0.4459 - val_accuracy: 0.8652\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2675 - accuracy: 0.9126 - val_loss: 0.4425 - val_accuracy: 0.8611\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2888 - accuracy: 0.9047 - val_loss: 0.4252 - val_accuracy: 0.8666\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2830 - accuracy: 0.9071 - val_loss: 0.3977 - val_accuracy: 0.8734\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2547 - accuracy: 0.9139 - val_loss: 0.4208 - val_accuracy: 0.8741\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2660 - accuracy: 0.9104 - val_loss: 0.3850 - val_accuracy: 0.8797\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2752 - accuracy: 0.9084 - val_loss: 0.3797 - val_accuracy: 0.8811\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2805 - accuracy: 0.9048 - val_loss: 0.4129 - val_accuracy: 0.8719\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2659 - accuracy: 0.9123 - val_loss: 0.3755 - val_accuracy: 0.8827\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2666 - accuracy: 0.9153 - val_loss: 0.3962 - val_accuracy: 0.8715\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2685 - accuracy: 0.9098 - val_loss: 0.4080 - val_accuracy: 0.8759\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2713 - accuracy: 0.9070 - val_loss: 0.3728 - val_accuracy: 0.8830\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2630 - accuracy: 0.9129 - val_loss: 0.3974 - val_accuracy: 0.8804\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2456 - accuracy: 0.9169 - val_loss: 0.4016 - val_accuracy: 0.8754\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2589 - accuracy: 0.9130 - val_loss: 0.4092 - val_accuracy: 0.8730\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2662 - accuracy: 0.9122 - val_loss: 0.4554 - val_accuracy: 0.8575\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2521 - accuracy: 0.9151 - val_loss: 0.4385 - val_accuracy: 0.8677\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2607 - accuracy: 0.9130 - val_loss: 0.3828 - val_accuracy: 0.8858\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2619 - accuracy: 0.9148 - val_loss: 0.4196 - val_accuracy: 0.8755\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2586 - accuracy: 0.9129 - val_loss: 0.3741 - val_accuracy: 0.8807\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2501 - accuracy: 0.9184 - val_loss: 0.4218 - val_accuracy: 0.8709\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2391 - accuracy: 0.9210 - val_loss: 0.3772 - val_accuracy: 0.8818\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2575 - accuracy: 0.9144 - val_loss: 0.3721 - val_accuracy: 0.8874\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2580 - accuracy: 0.9140 - val_loss: 0.3823 - val_accuracy: 0.8836\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2302 - accuracy: 0.9241 - val_loss: 0.3786 - val_accuracy: 0.8825\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2378 - accuracy: 0.9199 - val_loss: 0.4286 - val_accuracy: 0.8712\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2625 - accuracy: 0.9139 - val_loss: 0.3461 - val_accuracy: 0.8901\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2467 - accuracy: 0.9177 - val_loss: 0.3875 - val_accuracy: 0.8823\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2325 - accuracy: 0.9232 - val_loss: 0.4280 - val_accuracy: 0.8732\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2496 - accuracy: 0.9189 - val_loss: 0.4038 - val_accuracy: 0.8780\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2524 - accuracy: 0.9141 - val_loss: 0.3542 - val_accuracy: 0.8856\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2342 - accuracy: 0.9251 - val_loss: 0.4109 - val_accuracy: 0.8776\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.2382 - accuracy: 0.9216 - val_loss: 0.4088 - val_accuracy: 0.8777\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2301 - accuracy: 0.9257 - val_loss: 0.3611 - val_accuracy: 0.8889\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.2354 - accuracy: 0.9224 - val_loss: 0.4033 - val_accuracy: 0.8733\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2418 - accuracy: 0.9195 - val_loss: 0.4058 - val_accuracy: 0.8786\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.2203 - accuracy: 0.9273 - val_loss: 0.4395 - val_accuracy: 0.8710\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.2162 - accuracy: 0.9285 - val_loss: 0.3335 - val_accuracy: 0.8978\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1967 - accuracy: 0.9359 - val_loss: 0.3244 - val_accuracy: 0.8979\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1745 - accuracy: 0.9437 - val_loss: 0.3229 - val_accuracy: 0.8997\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1631 - accuracy: 0.9478 - val_loss: 0.3220 - val_accuracy: 0.9001\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1573 - accuracy: 0.9513 - val_loss: 0.3238 - val_accuracy: 0.9013\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1588 - accuracy: 0.9477 - val_loss: 0.3124 - val_accuracy: 0.9038\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1495 - accuracy: 0.9545 - val_loss: 0.3172 - val_accuracy: 0.9016\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1492 - accuracy: 0.9531 - val_loss: 0.3217 - val_accuracy: 0.9004\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1515 - accuracy: 0.9497 - val_loss: 0.3177 - val_accuracy: 0.9022\n",
      "Epoch 160/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1575 - accuracy: 0.9500 - val_loss: 0.3134 - val_accuracy: 0.9023\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1528 - accuracy: 0.9527 - val_loss: 0.3176 - val_accuracy: 0.9011\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1460 - accuracy: 0.9559 - val_loss: 0.3102 - val_accuracy: 0.9036\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1458 - accuracy: 0.9548 - val_loss: 0.3139 - val_accuracy: 0.9041\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1450 - accuracy: 0.9546 - val_loss: 0.3068 - val_accuracy: 0.9061\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1455 - accuracy: 0.9553 - val_loss: 0.3150 - val_accuracy: 0.9060\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1471 - accuracy: 0.9535 - val_loss: 0.3063 - val_accuracy: 0.9043\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1465 - accuracy: 0.9540 - val_loss: 0.3112 - val_accuracy: 0.9039\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1417 - accuracy: 0.9566 - val_loss: 0.3176 - val_accuracy: 0.9033\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1371 - accuracy: 0.9577 - val_loss: 0.3144 - val_accuracy: 0.9025\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1352 - accuracy: 0.9583 - val_loss: 0.3200 - val_accuracy: 0.9027\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1441 - accuracy: 0.9553 - val_loss: 0.3129 - val_accuracy: 0.9042\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.1414 - accuracy: 0.9560 - val_loss: 0.3136 - val_accuracy: 0.9063\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1336 - accuracy: 0.9589 - val_loss: 0.3080 - val_accuracy: 0.9075\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1317 - accuracy: 0.9584 - val_loss: 0.3110 - val_accuracy: 0.9069\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1291 - accuracy: 0.9608 - val_loss: 0.3133 - val_accuracy: 0.9073\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.1387 - accuracy: 0.9581 - val_loss: 0.3130 - val_accuracy: 0.9067\n",
      "Epoch 177/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.1289 - accuracy: 0.9613 - val_loss: 0.3094 - val_accuracy: 0.9077\n",
      "Epoch 178/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1309 - accuracy: 0.9605 - val_loss: 0.3077 - val_accuracy: 0.9082\n",
      "Epoch 179/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1348 - accuracy: 0.9571 - val_loss: 0.3109 - val_accuracy: 0.9066\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1306 - accuracy: 0.9578 - val_loss: 0.3101 - val_accuracy: 0.9063\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1292 - accuracy: 0.9594 - val_loss: 0.3165 - val_accuracy: 0.9066\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.1271 - accuracy: 0.9609 - val_loss: 0.3134 - val_accuracy: 0.9065\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1334 - accuracy: 0.9576 - val_loss: 0.3072 - val_accuracy: 0.9062\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.1280 - accuracy: 0.9602 - val_loss: 0.3129 - val_accuracy: 0.9054\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.1261 - accuracy: 0.9617 - val_loss: 0.3094 - val_accuracy: 0.9065\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1222 - accuracy: 0.9625 - val_loss: 0.3163 - val_accuracy: 0.9069\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1306 - accuracy: 0.9591 - val_loss: 0.3114 - val_accuracy: 0.9086\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1275 - accuracy: 0.9609 - val_loss: 0.3100 - val_accuracy: 0.9065\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1261 - accuracy: 0.9626 - val_loss: 0.3154 - val_accuracy: 0.9057\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1262 - accuracy: 0.9610 - val_loss: 0.3190 - val_accuracy: 0.9030\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1265 - accuracy: 0.9595 - val_loss: 0.3179 - val_accuracy: 0.9072\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1281 - accuracy: 0.9597 - val_loss: 0.3089 - val_accuracy: 0.9081\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1256 - accuracy: 0.9606 - val_loss: 0.3203 - val_accuracy: 0.9054\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1353 - accuracy: 0.9579 - val_loss: 0.3152 - val_accuracy: 0.9074\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1268 - accuracy: 0.9598 - val_loss: 0.3143 - val_accuracy: 0.9063\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1245 - accuracy: 0.9615 - val_loss: 0.3095 - val_accuracy: 0.9079\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1238 - accuracy: 0.9634 - val_loss: 0.3245 - val_accuracy: 0.9035\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1232 - accuracy: 0.9615 - val_loss: 0.3215 - val_accuracy: 0.9052\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1226 - accuracy: 0.9630 - val_loss: 0.3222 - val_accuracy: 0.9059\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1229 - accuracy: 0.9620 - val_loss: 0.3231 - val_accuracy: 0.9047\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1222 - accuracy: 0.9632 - val_loss: 0.3238 - val_accuracy: 0.9050\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1231 - accuracy: 0.9620 - val_loss: 0.3160 - val_accuracy: 0.9070\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1302 - accuracy: 0.9582 - val_loss: 0.3128 - val_accuracy: 0.9062\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1181 - accuracy: 0.9648 - val_loss: 0.3167 - val_accuracy: 0.9077\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1182 - accuracy: 0.9638 - val_loss: 0.3156 - val_accuracy: 0.9067\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1208 - accuracy: 0.9618 - val_loss: 0.3215 - val_accuracy: 0.9051\n",
      "Epoch 207/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1196 - accuracy: 0.9616 - val_loss: 0.3223 - val_accuracy: 0.9059\n",
      "Epoch 208/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1219 - accuracy: 0.9613 - val_loss: 0.3196 - val_accuracy: 0.9082\n",
      "Epoch 209/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1183 - accuracy: 0.9631 - val_loss: 0.3190 - val_accuracy: 0.9089\n",
      "Epoch 210/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1179 - accuracy: 0.9642 - val_loss: 0.3171 - val_accuracy: 0.9076\n",
      "Epoch 211/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1273 - accuracy: 0.9594 - val_loss: 0.3148 - val_accuracy: 0.9069\n",
      "Epoch 212/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1156 - accuracy: 0.9637 - val_loss: 0.3144 - val_accuracy: 0.9066\n",
      "Epoch 213/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1167 - accuracy: 0.9648 - val_loss: 0.3180 - val_accuracy: 0.9079\n",
      "Epoch 214/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1210 - accuracy: 0.9635 - val_loss: 0.3239 - val_accuracy: 0.9060\n",
      "Epoch 215/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1143 - accuracy: 0.9670 - val_loss: 0.3225 - val_accuracy: 0.9055\n",
      "Epoch 216/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1201 - accuracy: 0.9609 - val_loss: 0.3196 - val_accuracy: 0.9057\n",
      "Epoch 217/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1185 - accuracy: 0.9627 - val_loss: 0.3175 - val_accuracy: 0.9061\n",
      "Epoch 218/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1186 - accuracy: 0.9624 - val_loss: 0.3226 - val_accuracy: 0.9047\n",
      "Epoch 219/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1129 - accuracy: 0.9640 - val_loss: 0.3250 - val_accuracy: 0.9055\n",
      "Epoch 220/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1144 - accuracy: 0.9647 - val_loss: 0.3242 - val_accuracy: 0.9052\n",
      "Epoch 221/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1152 - accuracy: 0.9641 - val_loss: 0.3214 - val_accuracy: 0.9062\n",
      "Epoch 222/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1109 - accuracy: 0.9666 - val_loss: 0.3222 - val_accuracy: 0.9060\n",
      "Epoch 223/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1136 - accuracy: 0.9676 - val_loss: 0.3208 - val_accuracy: 0.9063\n",
      "Epoch 224/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1099 - accuracy: 0.9663 - val_loss: 0.3213 - val_accuracy: 0.9065\n",
      "Epoch 225/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1171 - accuracy: 0.9641 - val_loss: 0.3235 - val_accuracy: 0.9045\n",
      "Epoch 226/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1202 - accuracy: 0.9624 - val_loss: 0.3274 - val_accuracy: 0.9042\n",
      "Epoch 227/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1120 - accuracy: 0.9651 - val_loss: 0.3238 - val_accuracy: 0.9044\n",
      "Epoch 228/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1084 - accuracy: 0.9668 - val_loss: 0.3194 - val_accuracy: 0.9056\n",
      "Epoch 229/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1064 - accuracy: 0.9674 - val_loss: 0.3223 - val_accuracy: 0.9061\n",
      "Epoch 230/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1050 - accuracy: 0.9667 - val_loss: 0.3245 - val_accuracy: 0.9050\n",
      "Epoch 231/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1085 - accuracy: 0.9659 - val_loss: 0.3150 - val_accuracy: 0.9085\n",
      "Epoch 232/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1023 - accuracy: 0.9700 - val_loss: 0.3215 - val_accuracy: 0.9064\n",
      "Epoch 233/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1124 - accuracy: 0.9638 - val_loss: 0.3210 - val_accuracy: 0.9073\n",
      "Epoch 234/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1056 - accuracy: 0.9675 - val_loss: 0.3181 - val_accuracy: 0.9075\n",
      "Epoch 235/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1095 - accuracy: 0.9675 - val_loss: 0.3173 - val_accuracy: 0.9081\n",
      "Epoch 236/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1057 - accuracy: 0.9685 - val_loss: 0.3248 - val_accuracy: 0.9049\n",
      "Epoch 237/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1121 - accuracy: 0.9666 - val_loss: 0.3234 - val_accuracy: 0.9062\n",
      "Epoch 238/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1076 - accuracy: 0.9682 - val_loss: 0.3243 - val_accuracy: 0.9058\n",
      "Epoch 239/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1079 - accuracy: 0.9669 - val_loss: 0.3190 - val_accuracy: 0.9082\n",
      "Epoch 240/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1105 - accuracy: 0.9663 - val_loss: 0.3211 - val_accuracy: 0.9071\n",
      "Epoch 241/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1054 - accuracy: 0.9660 - val_loss: 0.3166 - val_accuracy: 0.9086\n",
      "Epoch 242/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1128 - accuracy: 0.9661 - val_loss: 0.3214 - val_accuracy: 0.9073\n",
      "Epoch 243/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1038 - accuracy: 0.9680 - val_loss: 0.3219 - val_accuracy: 0.9063\n",
      "Epoch 244/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1111 - accuracy: 0.9655 - val_loss: 0.3154 - val_accuracy: 0.9082\n",
      "Epoch 245/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1043 - accuracy: 0.9674 - val_loss: 0.3157 - val_accuracy: 0.9082\n",
      "Epoch 246/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1053 - accuracy: 0.9677 - val_loss: 0.3207 - val_accuracy: 0.9069\n",
      "Epoch 247/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1043 - accuracy: 0.9699 - val_loss: 0.3260 - val_accuracy: 0.9063\n",
      "Epoch 248/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1050 - accuracy: 0.9677 - val_loss: 0.3184 - val_accuracy: 0.9079\n",
      "Epoch 249/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1038 - accuracy: 0.9684 - val_loss: 0.3223 - val_accuracy: 0.9072\n",
      "Epoch 250/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1082 - accuracy: 0.9676 - val_loss: 0.3171 - val_accuracy: 0.9082\n",
      "Epoch 251/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1053 - accuracy: 0.9685 - val_loss: 0.3151 - val_accuracy: 0.9082\n",
      "Epoch 252/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1080 - accuracy: 0.9666 - val_loss: 0.3220 - val_accuracy: 0.9068\n",
      "Epoch 253/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1048 - accuracy: 0.9693 - val_loss: 0.3180 - val_accuracy: 0.9074\n",
      "Epoch 254/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1046 - accuracy: 0.9682 - val_loss: 0.3143 - val_accuracy: 0.9082\n",
      "Epoch 255/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1063 - accuracy: 0.9683 - val_loss: 0.3176 - val_accuracy: 0.9074\n",
      "Epoch 256/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1084 - accuracy: 0.9663 - val_loss: 0.3165 - val_accuracy: 0.9083\n",
      "Epoch 257/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1039 - accuracy: 0.9679 - val_loss: 0.3155 - val_accuracy: 0.9075\n",
      "Epoch 258/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1069 - accuracy: 0.9680 - val_loss: 0.3251 - val_accuracy: 0.9070\n",
      "Epoch 259/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1073 - accuracy: 0.9686 - val_loss: 0.3206 - val_accuracy: 0.9078\n",
      "Epoch 260/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1118 - accuracy: 0.9669 - val_loss: 0.3159 - val_accuracy: 0.9078\n",
      "Epoch 261/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1036 - accuracy: 0.9688 - val_loss: 0.3220 - val_accuracy: 0.9074\n",
      "Epoch 262/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1049 - accuracy: 0.9679 - val_loss: 0.3229 - val_accuracy: 0.9064\n",
      "Epoch 263/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1067 - accuracy: 0.9677 - val_loss: 0.3229 - val_accuracy: 0.9069\n",
      "Epoch 264/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1020 - accuracy: 0.9691 - val_loss: 0.3152 - val_accuracy: 0.9078\n",
      "Epoch 265/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1004 - accuracy: 0.9696 - val_loss: 0.3172 - val_accuracy: 0.9070\n",
      "Epoch 266/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1009 - accuracy: 0.9700 - val_loss: 0.3212 - val_accuracy: 0.9073\n",
      "Epoch 267/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1038 - accuracy: 0.9689 - val_loss: 0.3190 - val_accuracy: 0.9079\n",
      "Epoch 268/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1087 - accuracy: 0.9653 - val_loss: 0.3210 - val_accuracy: 0.9063\n",
      "Epoch 269/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1055 - accuracy: 0.9702 - val_loss: 0.3208 - val_accuracy: 0.9063\n",
      "Epoch 270/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1097 - accuracy: 0.9659 - val_loss: 0.3202 - val_accuracy: 0.9065\n",
      "Epoch 271/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1015 - accuracy: 0.9704 - val_loss: 0.3149 - val_accuracy: 0.9087\n",
      "Epoch 272/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1023 - accuracy: 0.9689 - val_loss: 0.3191 - val_accuracy: 0.9077\n",
      "Epoch 273/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1063 - accuracy: 0.9686 - val_loss: 0.3175 - val_accuracy: 0.9072\n",
      "Epoch 274/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1090 - accuracy: 0.9666 - val_loss: 0.3183 - val_accuracy: 0.9069\n",
      "Epoch 275/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1071 - accuracy: 0.9682 - val_loss: 0.3207 - val_accuracy: 0.9067\n",
      "Epoch 276/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1030 - accuracy: 0.9718 - val_loss: 0.3175 - val_accuracy: 0.9066\n",
      "Epoch 277/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1059 - accuracy: 0.9680 - val_loss: 0.3188 - val_accuracy: 0.9059\n",
      "Epoch 278/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1080 - accuracy: 0.9682 - val_loss: 0.3199 - val_accuracy: 0.9061\n",
      "Epoch 279/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1040 - accuracy: 0.9695 - val_loss: 0.3163 - val_accuracy: 0.9079\n",
      "Epoch 280/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1029 - accuracy: 0.9709 - val_loss: 0.3175 - val_accuracy: 0.9081\n",
      "Epoch 281/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1007 - accuracy: 0.9696 - val_loss: 0.3170 - val_accuracy: 0.9086\n",
      "Epoch 282/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1028 - accuracy: 0.9692 - val_loss: 0.3212 - val_accuracy: 0.9083\n",
      "Epoch 283/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0985 - accuracy: 0.9723 - val_loss: 0.3206 - val_accuracy: 0.9068\n",
      "Epoch 284/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1086 - accuracy: 0.9674 - val_loss: 0.3244 - val_accuracy: 0.9071\n",
      "Epoch 285/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1017 - accuracy: 0.9709 - val_loss: 0.3157 - val_accuracy: 0.9079\n",
      "Epoch 286/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1034 - accuracy: 0.9686 - val_loss: 0.3151 - val_accuracy: 0.9078\n",
      "Epoch 287/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1012 - accuracy: 0.9694 - val_loss: 0.3199 - val_accuracy: 0.9067\n",
      "Epoch 288/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1070 - accuracy: 0.9666 - val_loss: 0.3173 - val_accuracy: 0.9080\n",
      "Epoch 289/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1046 - accuracy: 0.9671 - val_loss: 0.3161 - val_accuracy: 0.9086\n",
      "Epoch 290/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1013 - accuracy: 0.9686 - val_loss: 0.3214 - val_accuracy: 0.9060\n",
      "Epoch 291/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0985 - accuracy: 0.9712 - val_loss: 0.3168 - val_accuracy: 0.9072\n",
      "Epoch 292/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1051 - accuracy: 0.9680 - val_loss: 0.3167 - val_accuracy: 0.9082\n",
      "Epoch 293/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1090 - accuracy: 0.9658 - val_loss: 0.3215 - val_accuracy: 0.9069\n",
      "Epoch 294/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0990 - accuracy: 0.9702 - val_loss: 0.3181 - val_accuracy: 0.9066\n",
      "Epoch 295/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1014 - accuracy: 0.9705 - val_loss: 0.3197 - val_accuracy: 0.9071\n",
      "Epoch 296/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.1081 - accuracy: 0.9684 - val_loss: 0.3204 - val_accuracy: 0.9070\n",
      "Epoch 297/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.1047 - accuracy: 0.9700 - val_loss: 0.3171 - val_accuracy: 0.9072\n",
      "Epoch 298/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.1003 - accuracy: 0.9711 - val_loss: 0.3204 - val_accuracy: 0.9069\n",
      "Epoch 299/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.1115 - accuracy: 0.9656 - val_loss: 0.3198 - val_accuracy: 0.9068\n",
      "Epoch 300/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0993 - accuracy: 0.9692 - val_loss: 0.3217 - val_accuracy: 0.9064\n",
      "Wall time: 5h 8min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x205b0270f28>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit_generator(datagen.flow(X_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=nb_epochs,\n",
    "                        validation_data=(X_test, y_test),callbacks=cbks,steps_per_epoch=iterations,\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 14s 1ms/step\n",
      "Accuracy = 90.640002\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test,verbose = 1)\n",
    "print(\"Accuracy = %f\" % (100 * scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
